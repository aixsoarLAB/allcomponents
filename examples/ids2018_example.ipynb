{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 將包含 deeplog 模組的目錄路徑添加到 sys.path\n",
    "sys.path.append('../DeepLog/deeplog')\n",
    "# import DeepLog and Preprocessor\n",
    "from deeplog              import DeepLog\n",
    "from preprocessor import Preprocessor\n",
    "# Import pytorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[47, 47, 47,  ..., 47, 47, 47],\n",
      "        [47, 47, 47,  ..., 47, 47, 37],\n",
      "        [47, 47, 47,  ..., 47, 37, 34],\n",
      "        ...,\n",
      "        [36, 37, 34,  ..., 34, 36, 37],\n",
      "        [37, 34, 36,  ..., 36, 37, 37],\n",
      "        [34, 36, 37,  ..., 37, 37, 37]]) \n",
      "Shape: torch.Size([9034, 20]) \n",
      "mapping: {0: 1, 1: 2, 2: 6, 3: 12, 4: 13, 5: 14, 6: 16, 7: 18, 8: 19, 9: 20, 10: 25, 11: 27, 12: 32, 13: 35, 14: 37, 15: 43, 16: 44, 17: 55, 18: 98, 19: 109, 20: 153, 21: 172, 22: 1014, 23: 1074, 24: 4200, 25: 6005, 26: 6006, 27: 6009, 28: 6013, 29: 7000, 30: 7001, 31: 7002, 32: 7009, 33: 7023, 34: 7024, 35: 7026, 36: 7031, 37: 7036, 38: 7040, 39: 7045, 40: 10016, 41: 10148, 42: 16962, 43: 50036, 44: 50037, 45: 51046, 46: 51047, 47: -1337}\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                                 Load data                                  #\n",
    "##############################################################################\n",
    "\n",
    "# Create preprocessor for cd loading data\n",
    "preprocessor = Preprocessor(\n",
    "    length  = 20,           # Extract sequences of 20 items\n",
    "    timeout = float('inf'), # Do not include a maximum allowed time between events\n",
    ")\n",
    "\n",
    "training_data_path = \"./data/IDS2018_train_Benign\"\n",
    "\n",
    "# Load data from csv file\n",
    "#X, y, label, mapping = preprocessor.csv(training_data_path)\n",
    "# Load data from txt file\n",
    "X, y, label, mapping = preprocessor.text(training_data_path)\n",
    "\n",
    "print(\"X:\", X, \"\\nShape:\", X.shape, \"\\nmapping:\", mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1/10] average loss = 4.4885 ######################################## (100.00%) runtime 0:00:17.2\n",
      "[Epoch  2/10] average loss = 4.1647 ######################################## (100.00%) runtime 0:00:15.5\n",
      "[Epoch  3/10] average loss = 3.7711 ######################################## (100.00%) runtime 0:00:14.3\n",
      "[Epoch  4/10] average loss = 3.2063 ######################################## (100.00%) runtime 0:00:15.8\n",
      "[Epoch  5/10] average loss = 2.4081 ######################################## (100.00%) runtime 0:00:14.8\n",
      "[Epoch  6/10] average loss = 1.7602 ######################################## (100.00%) runtime 0:00:15.5\n",
      "[Epoch  7/10] average loss = 1.5042 ######################################## (100.00%) runtime 0:00:14.4\n",
      "[Epoch  8/10] average loss = 1.4290 ######################################## (100.00%) runtime 0:00:14.8\n",
      "[Epoch  9/10] average loss = 1.4044 ######################################## (100.00%) runtime 0:00:16.6\n",
      "[Epoch 10/10] average loss = 1.3944 ######################################## (100.00%) runtime 0:00:14.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepLog(\n",
       "  (lstm): LSTM(60, 64, num_layers=2, batch_first=True)\n",
       "  (out): Linear(in_features=64, out_features=100, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                                  DeepLog                                   #\n",
    "##############################################################################\n",
    "\n",
    "# Create DeepLog object\n",
    "deeplog = DeepLog(\n",
    "    input_size  = 60, # Number of different events to expect\n",
    "    hidden_size = 64 , # Hidden dimension, we suggest 64\n",
    "    output_size = 100, # Number of different events to expect\n",
    ")\n",
    "\n",
    "# Optionally cast data and DeepLog to cuda, if available\n",
    "if torch.cuda.is_available():\n",
    "    deeplog = deeplog.to(\"cuda\")\n",
    "    X       = X      .to(\"cuda\")\n",
    "    y       = y      .to(\"cuda\")\n",
    "\n",
    "# Train deeplog\n",
    "deeplog.fit(\n",
    "    X          = X,\n",
    "    y          = y,\n",
    "    epochs     = 10,\n",
    "    batch_size = 128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xp: tensor([[41, 41, 41,  ..., 41, 41, 41],\n",
      "        [41, 41, 41,  ..., 41, 41, 34],\n",
      "        [41, 41, 41,  ..., 41, 41, 41],\n",
      "        ...,\n",
      "        [30, 31, 28,  ..., 28, 30, 31],\n",
      "        [31, 28, 30,  ..., 30, 31, 31],\n",
      "        [28, 30, 31,  ..., 31, 31, 28]]) \n",
      "Shape: torch.Size([9034, 20]) \n",
      "mapping_p: {0: 1, 1: 2, 2: 6, 3: 12, 4: 13, 5: 14, 6: 18, 7: 19, 8: 20, 9: 25, 10: 27, 11: 32, 12: 43, 13: 44, 14: 55, 15: 98, 16: 109, 17: 153, 18: 172, 19: 1074, 20: 4200, 21: 6005, 22: 6006, 23: 6009, 24: 6013, 25: 7001, 26: 7002, 27: 7023, 28: 7024, 29: 7026, 30: 7031, 31: 7036, 32: 7040, 33: 7045, 34: 10016, 35: 10148, 36: 16962, 37: 50036, 38: 50037, 39: 51046, 40: 51047, 41: -1337}\n",
      "[Epoch 1/1] average loss = 0.0000 ######################################## (100.00%) runtime 0:00:04.8\n",
      "y_pred: tensor([[37, 36, 34,  ..., 46, 31, 99],\n",
      "        [37, 36, 34,  ..., 46, 14, 31],\n",
      "        [37, 36, 34,  ..., 46, 31, 99],\n",
      "        ...,\n",
      "        [37, 36, 34,  ..., 46, 14, 31],\n",
      "        [37, 36, 34,  ..., 46, 14, 31],\n",
      "        [37, 36, 34,  ..., 46, 14, 31]]) \n",
      "shape: torch.Size([903, 9])\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                                  Predict                                   #\n",
    "##############################################################################\n",
    "\n",
    "data_path = \"./data/IDS2018_test_Benign\"\n",
    "\n",
    "# Load data from csv file\n",
    "#Xp, yp, label, mapping_p = preprocessor.csv(\"/home/ubuntu/DeepLog/examples/data/hdfs_train\")\n",
    "# Load data from txt file\n",
    "Xp, yp, label, mapping_p = preprocessor.text(data_path)\n",
    "print(\"Xp:\", Xp, \"\\nShape:\", X.shape, \"\\nmapping_p:\", mapping_p)\n",
    "\n",
    "# Predict using deeplog\n",
    "y_pred, confidence = deeplog.predict(\n",
    "    X = Xp,\n",
    "    k = 9,\n",
    ")\n",
    "\n",
    "print(\"y_pred:\", y_pred, \"\\nshape:\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['10016', '10016', '10016', '7036', '7036', '7036', '7036', '6013', '1', '1', '7036', '7036', '7036', '1074', '10016', '6006', '7036', '7036', '7002', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '51047', '50037', '7036', '7036', '7036', '7036', '6009', '6005', '6013', '7036', '7036', '7036', '7036', '7036', '7036', '109', '13', '12', '153', '20', '27', '25', '18', '32', '6', '6', '98', '6', '6', '172', '55', '14', '16962', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '6', '6', '6', '7036', '7036', '7036', '7036', '7036', '7036', '50036', '7036', '7036', '7036', '51046', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '10148', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7001', '7036', '7036', '1', '7036', '7036', '4200', '6', '7026', '7036', '7036', '7036', '7036', '7036', '7036', '7024', '7031', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7036', '7040', '44', '7036', '7036', '7036', '7036', '7036', '7023', '7045', '7036', '43', '7036', '7024', '7031', '7045', '19', '7040', '7036', '7036', '7024', '7031', '7036', '7036', '7036', '7023', '7036', '7023', '7036', '7023', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '2', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '6013', '6013', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7036', '7036', '7036', '7036', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7024', '7031', '7036', '7036', '7036', '7036', '7036', '7036', '7024', '7031', '7036', '7036', '7024', '7031', '7036', '7036', '7024', '7031'] \n",
      "size: 903\n",
      "{1: 0, 2: 1, 6: 2, 12: 3, 13: 4, 14: 5, 18: 6, 19: 7, 20: 8, 25: 9, 27: 10, 32: 11, 43: 12, 44: 13, 55: 14, 98: 15, 109: 16, 153: 17, 172: 18, 1074: 19, 4200: 20, 6005: 21, 6006: 22, 6009: 23, 6013: 24, 7001: 25, 7002: 26, 7023: 27, 7024: 28, 7026: 29, 7031: 30, 7036: 31, 7040: 32, 7045: 33, 10016: 34, 10148: 35, 16962: 36, 50036: 37, 50037: 38, 51046: 39, 51047: 40, -1337: 41}\n",
      "mapped_data: [34, 34, 34, 31, 31, 31, 31, 24, 0, 0, 31, 31, 31, 19, 34, 22, 31, 31, 26, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 40, 38, 31, 31, 31, 31, 23, 21, 24, 31, 31, 31, 31, 31, 31, 16, 4, 3, 17, 8, 10, 9, 6, 11, 2, 2, 15, 2, 2, 18, 14, 5, 36, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 2, 2, 2, 31, 31, 31, 31, 31, 31, 37, 31, 31, 31, 39, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 35, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 25, 31, 31, 0, 31, 31, 20, 2, 29, 31, 31, 31, 31, 31, 31, 28, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 31, 31, 32, 13, 31, 31, 31, 31, 31, 27, 33, 31, 12, 31, 28, 30, 33, 7, 32, 31, 31, 28, 30, 31, 31, 31, 27, 31, 27, 31, 27, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 1, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 31, 31, 28, 30, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 24, 24, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 31, 31, 31, 31, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 28, 30, 31, 31, 31, 31, 31, 31, 28, 30, 31, 31, 28, 30, 31, 31, 28, 30] \n",
      "size: 903\n",
      "results: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0] \n",
      "size: 903\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                                 Comparison                                 #\n",
    "##############################################################################\n",
    "\n",
    "# 讀取測試文件\n",
    "with open(data_path, 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "cleaned_data = [item.strip() for item in data]\n",
    "\n",
    "# 使用列表推導式分隔每個字串，並將結果扁平化形成一個新的串列\n",
    "data = [item for sublist in cleaned_data for item in sublist.split()]\n",
    "\n",
    "print(\"data:\", data, \"\\nsize:\", len(data))\n",
    "\n",
    "# 反轉 mapping，以便我們可以根據事件ID找到對應的編號\n",
    "reverse_mapping = {v: k for k, v in mapping_p.items()}\n",
    "print(reverse_mapping)\n",
    "\n",
    "# 轉換 events 列表中的每個字串為數字，然後根據 reverse_mapping 進行映射\n",
    "mapped_data = [reverse_mapping[int(event)] for event in data]\n",
    "\n",
    "print(\"mapped_data:\", mapped_data, \"\\nsize:\", len(mapped_data))\n",
    "\n",
    "# 初始化一個零張量，用於存儲比較結果，長度與 test_normal_data 相同\n",
    "results = []\n",
    "\n",
    "# 遍歷 test_normal_data 的每一行\n",
    "for i in range(0, len(mapped_data)):\n",
    "    match = False\n",
    "    for j in y_pred[i]:\n",
    "        if mapped_data[i] == j:\n",
    "            match = True\n",
    "            break\n",
    "    \n",
    "    # 如果有匹配，設置結果為1\n",
    "    if match:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)\n",
    "\n",
    "print(\"results:\", results, \"\\nsize:\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "異常數量：517\n",
      "異常率：0.573\n"
     ]
    }
   ],
   "source": [
    "# 計算列表中0的數量\n",
    "num_of_zero = results.count(0)\n",
    "print(\"異常數量：%d\" %num_of_zero)\n",
    "\n",
    "# 計算機率\n",
    "abnormal_rate = num_of_zero / len(results)\n",
    "\n",
    "print(\"異常率：%.3f\" %abnormal_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
